{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# TSM模型进行视频分类\n",
    "## 所需环境\n",
    "\n",
    "有GPU就`paddlepaddle-gpu==2.2.1`,无GPU就`paddlepaddle==2.2.1`\n",
    "\n",
    "详情请看requirements.txt,文件具有一定兼容性.库的近似版本应该也可以.\n",
    "\n",
    "## 文件下载\n",
    "\n",
    "暴力数据集(二分类)可以在下面的连接下载(下载完请放入data文件夹下)\n",
    "\n",
    "https://aistudio.baidu.com/aistudio/datasetdetail/125525\n",
    "\n",
    "训练所需的预训练权重,可以在下面连接下载(下载完请放入项目的根目录下)\n",
    "\n",
    "https://videotag.bj.bcebos.com/PaddleVideo-release2.1/TSM/TSM_k400.pdparams\n",
    "\n",
    "## 训练步骤\n",
    "1. 数据集的准备\n",
    "下载暴力视频数据集,下载完请放入data文件夹下\n",
    "\n",
    "```\n",
    "数据集格式如下所示:\n",
    "├── data\n",
    "│   └── Violence\n",
    "│       ├── V_1.mp4\n",
    "│       ├── V_2.mp4\n",
    "│   └── NonViolence\n",
    "│       ├── NV_1.mp4\n",
    "│       ├── NV_2.mp4\n",
    "```\n",
    "\n",
    "2. 下载预训练权重\n",
    "下载完请放入项目的根目录下\n",
    "\n",
    "3. 数据集的处理\n",
    "运行`get_annotation.py`文件,获得数据集的索引文件,在annotation下生成.具体生成格式如下所示:\n",
    "```\n",
    "data/data/NonViolence/NV_296.mp4 0\n",
    "data/data/NonViolence/NV_462.mp4 0\n",
    "data/data/NonViolence/NV_985.mp4 0\n",
    "```\n",
    "4. 训练步骤\n",
    "运行`train.py`即可开始训练(如果没有GPU的话,建议把`setting.py`中的`log_interval`变量改成1,方便我们更自己的观看进度)\n",
    "\n",
    "训练中会在output中输出许多模型文件.\n",
    "\n",
    "5. 预测步骤\n",
    "首先在`predict.py`更改`model_file`变量指定自己的模型文件.\n",
    "\n",
    "然后运行`predict.py`即可开始预测(默认预测数据的验证集,如果想进行修改可以定义一个类似与`annotation/violence_val_videos.txt`)进行预测.\n",
    "\n",
    "## 我想直接运行你的代码\n",
    "请去Ai Studio直接fork,之后便可以运行我的代码了.\n",
    "\n",
    "项目连接:https://aistudio.baidu.com/aistudio/projectdetail/3415438\n",
    "\n",
    "## Q & A\n",
    "Q: 我想开启top5的计算,应该怎么做的\n",
    "\n",
    "A: 请取消`utils.py 134行`和`model.py 437行`的注释,注释掉`utils.py 92-93行`\n",
    "\n",
    "Q: 我使用的自己的数据集,为什么出现了下面的错误\n",
    "```\n",
    "SystemError: (Fatal) Blocking queue is killed because the data reader raises an exception.\n",
    "  [Hint: Expected killed_ != true, but received killed_:1 == true:1.] (at /paddle/paddle/fluid/operators/reader/blocking_queue.h:166)\n",
    "```\n",
    "A: 系统没有找到你的annoation中视频的文件,这里出现了问题\n",
    "```\n",
    "data/data/NonViolence/NV_296.mp4(没有找到) 0\n",
    "data/data/NonViolence/NV_462.mp4(没有找到) 0\n",
    "data/data/NonViolence/NV_985.mp4(没有找到) 0\n",
    "```\n",
    "\n",
    "Q: 跑着跑着程序被杀死是什么问题\n",
    "\n",
    "A: 很有可能是你的视频太大,例如(大于10MB),希望减小batch_size,实在不行删掉这个视频吧."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Ai Stdio 练丹开始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 解压文件\n",
    "!unzip -q data/data125597/data.zip -d data\n",
    "!mv data/data/* data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-01-15 21:17:37--  https://videotag.bj.bcebos.com/PaddleVideo-release2.1/TSM/TSM_k400.pdparams\n",
      "Resolving videotag.bj.bcebos.com (videotag.bj.bcebos.com)... 182.61.200.229, 182.61.200.195, 2409:8c04:1001:1002:0:ff:b001:368a\n",
      "Connecting to videotag.bj.bcebos.com (videotag.bj.bcebos.com)|182.61.200.229|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 147734703 (141M) [application/octet-stream]\n",
      "Saving to: ‘TSM_k400.pdparams.1’\n",
      "\n",
      "TSM_k400.pdparams.1 100%[===================>] 140.89M  30.7MB/s    in 7.2s    \n",
      "\n",
      "2022-01-15 21:17:44 (19.5 MB/s) - ‘TSM_k400.pdparams.1’ saved [147734703/147734703]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 下载预先训练权重\n",
    "!wget https://videotag.bj.bcebos.com/PaddleVideo-release2.1/TSM/TSM_k400.pdparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import copy\n",
    "import time\n",
    "import paddle\n",
    "import random\n",
    "import traceback\n",
    "\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "import paddle.nn.initializer as init\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from paddle import ParamAttr\n",
    "from collections import OrderedDict\n",
    "from collections.abc import Sequence\n",
    "from paddle.regularizer import L2Decay\n",
    "from paddle.nn import (Conv2D, BatchNorm2D, Linear, Dropout, MaxPool2D,\n",
    "                       AdaptiveAvgPool2D)\n",
    "\n",
    "from settings import *\n",
    "from data_preprocessing import *\n",
    "from model import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_model(validate=True):\n",
    "    \"\"\"Train model entry\n",
    "    Args:\n",
    "        weights (str): weights path for finetuning.\n",
    "        validate (bool): Whether to do evaluation. Default: False.\n",
    "    \"\"\"\n",
    "    output_dir = f\"./output/{model_name}\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # 1. Construct model\n",
    "    tsm = ResNetTSM(pretrained=pretrained,\n",
    "                    layers=layers,\n",
    "                    num_seg=num_seg)\n",
    "    head = TSMHead(num_classes=num_classes,\n",
    "                   in_channels=in_channels,\n",
    "                   drop_ratio=drop_ratio)\n",
    "    model = Recognizer2D(backbone=tsm, head=head)\n",
    "\n",
    "    # 2. Construct dataset and dataloader\n",
    "    train_pipeline = Compose(train_mode=True)\n",
    "    train_dataset = VideoDataset(file_path=train_file_path,\n",
    "                                 pipeline=train_pipeline,\n",
    "                                 suffix=suffix)\n",
    "    train_sampler = paddle.io.DistributedBatchSampler(train_dataset,\n",
    "                                                      batch_size=batch_size,\n",
    "                                                      shuffle=train_shuffle,\n",
    "                                                      drop_last=True)  \n",
    "\n",
    "    train_loader = paddle.io.DataLoader(train_dataset,\n",
    "                                        batch_sampler=train_sampler,\n",
    "                                        places=paddle.set_device(device),\n",
    "                                        return_list=return_list)\n",
    "    total_steps = len(train_dataset) / batch_size\n",
    "\n",
    "    if validate:\n",
    "        valid_pipeline = Compose(train_mode=False)\n",
    "        valid_dataset = VideoDataset(file_path=valid_file_path,\n",
    "                                     pipeline=valid_pipeline,\n",
    "                                     suffix=suffix)\n",
    "        valid_sampler = paddle.io.DistributedBatchSampler(valid_dataset,\n",
    "                                                          batch_size=batch_size,\n",
    "                                                          shuffle=valid_shuffle,\n",
    "                                                          drop_last=True)\n",
    "        valid_loader = paddle.io.DataLoader(valid_dataset,\n",
    "                                            batch_sampler=valid_sampler,\n",
    "                                            places=paddle.set_device(device),\n",
    "                                            return_list=return_list)\n",
    "\n",
    "    # 3. Construct solver.\n",
    "    lr = paddle.optimizer.lr.PiecewiseDecay(boundaries=boundaries, values=values)\n",
    "    optimizer = paddle.optimizer.Momentum(\n",
    "        learning_rate=lr,\n",
    "        momentum=momentum,\n",
    "        parameters=model.parameters(),\n",
    "        grad_clip=paddle.nn.ClipGradByGlobalNorm(clip_norm=clip_norm)\n",
    "    )\n",
    "\n",
    "    # 4. Train Model\n",
    "    best = 0.\n",
    "    for epoch in range(0, epochs):\n",
    "        model.train()\n",
    "        record_list = build_record(framework)\n",
    "        tic = time.time()\n",
    "        for i, data in enumerate(train_loader):\n",
    "            record_list['reader_time'].update(time.time() - tic)\n",
    "\n",
    "            # 4.1 forward\n",
    "            outputs = model.train_step(data)\n",
    "\n",
    "            # 4.2 backward\n",
    "            avg_loss = outputs['loss']\n",
    "            avg_loss.backward()\n",
    "\n",
    "            # 4.3 minimize\n",
    "            optimizer.step()\n",
    "            optimizer.clear_grad()\n",
    "\n",
    "            # log record\n",
    "            record_list['lr'].update(optimizer._global_learning_rate(), batch_size)\n",
    "            for name, value in outputs.items():\n",
    "                record_list[name].update(value, batch_size)\n",
    "\n",
    "            record_list['batch_time'].update(time.time() - tic)\n",
    "            tic = time.time()\n",
    "\n",
    "            if i % log_interval == 0:\n",
    "                ips = \"ips: {:.5f} instance/sec.\".format(\n",
    "                    batch_size / record_list[\"batch_time\"].val)\n",
    "                log_batch(record_list, i, epoch + 1, epochs, \"train\", ips, total_steps)\n",
    "\n",
    "        # learning rate epoch step\n",
    "        lr.step()\n",
    "\n",
    "        ips = \"avg_ips: {:.5f} instance/sec.\".format(\n",
    "            batch_size * record_list[\"batch_time\"].count /\n",
    "            record_list[\"batch_time\"].sum)\n",
    "        log_epoch(record_list, epoch + 1, \"train\", ips)\n",
    "\n",
    "        def evaluate(best):\n",
    "            model.eval()\n",
    "            record_list = build_record(framework)\n",
    "            record_list.pop('lr')\n",
    "            tic = time.time()\n",
    "            for i, data in enumerate(valid_loader):\n",
    "                outputs = model.val_step(data)\n",
    "\n",
    "                # log_record\n",
    "                for name, value in outputs.items():\n",
    "                    record_list[name].update(value, batch_size)\n",
    "\n",
    "                record_list['batch_time'].update(time.time() - tic)\n",
    "                tic = time.time()\n",
    "\n",
    "                if i % log_interval == 0:\n",
    "                    ips = \"ips: {:.5f} instance/sec.\".format(\n",
    "                        batch_size / record_list[\"batch_time\"].val)\n",
    "                    log_batch(record_list, i, epoch + 1, epochs, \"val\", ips, total_steps)\n",
    "\n",
    "            ips = \"avg_ips: {:.5f} instance/sec.\".format(\n",
    "                batch_size * record_list[\"batch_time\"].count /\n",
    "                record_list[\"batch_time\"].sum)\n",
    "            log_epoch(record_list, epoch + 1, \"val\", ips)\n",
    "\n",
    "            best_flag = False\n",
    "            for top_flag in ['hit_at_one', 'top1']:\n",
    "                if record_list.get(\n",
    "                        top_flag) and record_list[top_flag].avg > best:\n",
    "                    best = record_list[top_flag].avg\n",
    "                    best_flag = True\n",
    "            return best, best_flag\n",
    "\n",
    "        # 5. Validation\n",
    "        if validate or epoch == epochs - 1:\n",
    "            with paddle.no_grad():\n",
    "                best, save_best_flag = evaluate(best)\n",
    "            # save best\n",
    "            if save_best_flag:\n",
    "                paddle.save(optimizer.state_dict(),\n",
    "                     osp.join(output_dir, model_name + \"_best.pdopt\"))\n",
    "                paddle.save(model.state_dict(),\n",
    "                     osp.join(output_dir, model_name + \"_best.pdparams\"))\n",
    "                print(\n",
    "                    f\"Already save the best model (top1 acc){int(best *10000)/10000}\"\n",
    "                )\n",
    "\n",
    "        # 6. Save model and optimizer\n",
    "        if epoch % save_interval == 0 or epoch == epochs - 1:\n",
    "            paddle.save(\n",
    "                optimizer.state_dict(),\n",
    "                osp.join(output_dir,\n",
    "                         model_name + f\"_epoch_{epoch+1:05d}.pdopt\"))\n",
    "            paddle.save(\n",
    "                model.state_dict(),\n",
    "                osp.join(output_dir,\n",
    "                         model_name + f\"_epoch_{epoch+1:05d}.pdparams\"))\n",
    "\n",
    "    print(f'training {model_name} finished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0116 11:11:16.944785   179 device_context.cc:404] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 10.1, Runtime API Version: 10.1\n",
      "W0116 11:11:16.950973   179 device_context.cc:422] device: 0, cuDNN Version: 7.6.\n",
      "\n",
      "  0%|          | 0/265 [00:00<?, ?it/s]\n",
      "Loading conv._conv.weight: \u001b[A\n",
      "Loading conv._batch_norm.weight: \u001b[A\n",
      "Loading conv._batch_norm.bias:   \u001b[A\n",
      "Loading conv._batch_norm._mean: \u001b[A\n",
      "Loading conv._batch_norm._variance: \u001b[A\n",
      "Loading res2a.conv0._conv.weight:   \u001b[A\n",
      "Loading res2a.conv0._batch_norm.weight: \u001b[A\n",
      "  3%|▎         | 8/265 [00:00<00:03, 78.94it/s]\n",
      "Loading res2a.conv0._batch_norm._mean: \u001b[A\n",
      "Loading res2a.conv0._batch_norm._variance: \u001b[A\n",
      "Loading res2a.conv1._conv.weight:          \u001b[A\n",
      "Loading res2a.conv1._batch_norm.weight: \u001b[A\n",
      "Loading res2a.conv1._batch_norm.bias:   \u001b[A\n",
      "Loading res2a.conv1._batch_norm._mean: \u001b[A\n",
      "Loading res2a.conv1._batch_norm._variance: \u001b[A\n",
      "Loading res2a.conv2._conv.weight:          \u001b[A\n",
      "  6%|▋         | 17/265 [00:00<00:03, 80.38it/s]\n",
      "Loading res2a.conv2._batch_norm.bias:   \u001b[A\n",
      "Loading res2a.conv2._batch_norm._mean: \u001b[A\n",
      "Loading res2a.conv2._batch_norm._variance: \u001b[A\n",
      "Loading res2a.short._conv.weight:          \u001b[A\n",
      "Loading res2a.short._batch_norm.weight: \u001b[A\n",
      "Loading res2a.short._batch_norm.bias:   \u001b[A\n",
      "Loading res2a.short._batch_norm._mean: \u001b[A\n",
      "Loading res2a.short._batch_norm._variance: \u001b[A\n",
      " 10%|▉         | 26/265 [00:00<00:02, 81.47it/s]\n",
      "Loading res2b.conv0._batch_norm.weight: \u001b[A\n",
      "Loading res2b.conv0._batch_norm.bias:   \u001b[A\n",
      "Loading res2b.conv0._batch_norm._mean: \u001b[A\n",
      "Loading res2b.conv0._batch_norm._variance: \u001b[A\n",
      "Loading res2b.conv1._conv.weight:          \u001b[A\n",
      "Loading res2b.conv1._batch_norm.weight: \u001b[A\n",
      "Loading res2b.conv1._batch_norm.bias:   \u001b[A\n",
      "Loading res2b.conv1._batch_norm._mean: \u001b[A\n",
      " 13%|█▎        | 35/265 [00:00<00:02, 82.09it/s]\n",
      "Loading res2b.conv2._conv.weight:          \u001b[A\n",
      "Loading res2b.conv2._batch_norm.weight: \u001b[A\n",
      "Loading res2b.conv2._batch_norm.bias:   \u001b[A\n",
      "Loading res2b.conv2._batch_norm._mean: \u001b[A\n",
      "Loading res2b.conv2._batch_norm._variance: \u001b[A\n",
      "Loading res2c.conv0._conv.weight:          \u001b[A\n",
      "Loading res2c.conv0._batch_norm.weight: \u001b[A\n",
      "Loading res2c.conv0._batch_norm.bias:   \u001b[A\n",
      " 17%|█▋        | 44/265 [00:00<00:02, 82.48it/s]\n",
      "Loading res2c.conv0._batch_norm._variance: \u001b[A\n",
      "Loading res2c.conv1._conv.weight:          \u001b[A\n",
      "Loading res2c.conv1._batch_norm.weight: \u001b[A\n",
      "Loading res2c.conv1._batch_norm.bias:   \u001b[A\n",
      "Loading res2c.conv1._batch_norm._mean: \u001b[A\n",
      "Loading res2c.conv1._batch_norm._variance: \u001b[A\n",
      "Loading res2c.conv2._conv.weight:          \u001b[A\n",
      "Loading res2c.conv2._batch_norm.weight: \u001b[A\n",
      " 20%|██        | 53/265 [00:00<00:02, 82.54it/s]\n",
      "Loading res2c.conv2._batch_norm._mean: \u001b[A\n",
      "Loading res2c.conv2._batch_norm._variance: \u001b[A\n",
      "Loading res3a.conv0._conv.weight:          \u001b[A\n",
      "Loading res3a.conv0._batch_norm.weight: \u001b[A\n",
      "Loading res3a.conv0._batch_norm.bias:   \u001b[A\n",
      "Loading res3a.conv0._batch_norm._mean: \u001b[A\n",
      "Loading res3a.conv0._batch_norm._variance: \u001b[A\n",
      "Loading res3a.conv1._conv.weight:          \u001b[A\n",
      " 23%|██▎       | 62/265 [00:00<00:02, 82.73it/s]\n",
      "Loading res3a.conv1._batch_norm.bias:   \u001b[A\n",
      "Loading res3a.conv1._batch_norm._mean: \u001b[A\n",
      "Loading res3a.conv1._batch_norm._variance: \u001b[A\n",
      "Loading res3a.conv2._conv.weight:          \u001b[A\n",
      "Loading res3a.conv2._batch_norm.weight: \u001b[A\n",
      "Loading res3a.conv2._batch_norm.bias:   \u001b[A\n",
      "Loading res3a.conv2._batch_norm._mean: \u001b[A\n",
      "Loading res3a.conv2._batch_norm._variance: \u001b[A\n",
      " 27%|██▋       | 71/265 [00:00<00:02, 83.16it/s]\n",
      "Loading res3a.short._batch_norm.weight: \u001b[A\n",
      "Loading res3a.short._batch_norm.bias:   \u001b[A\n",
      "Loading res3a.short._batch_norm._mean: \u001b[A\n",
      "Loading res3a.short._batch_norm._variance: \u001b[A\n",
      "Loading res3b.conv0._conv.weight:          \u001b[A\n",
      "Loading res3b.conv0._batch_norm.weight: \u001b[A\n",
      "Loading res3b.conv0._batch_norm.bias:   \u001b[A\n",
      "Loading res3b.conv0._batch_norm._mean: \u001b[A\n",
      " 30%|███       | 80/265 [00:00<00:02, 84.52it/s]\n",
      "Loading res3b.conv1._conv.weight:          \u001b[A\n",
      "Loading res3b.conv1._batch_norm.weight: \u001b[A\n",
      "Loading res3b.conv1._batch_norm.bias:   \u001b[A\n",
      "Loading res3b.conv1._batch_norm._mean: \u001b[A\n",
      "Loading res3b.conv1._batch_norm._variance: \u001b[A\n",
      "Loading res3b.conv2._conv.weight:          \u001b[A\n",
      "Loading res3b.conv2._batch_norm.weight: \u001b[A\n",
      "Loading res3b.conv2._batch_norm.bias:   \u001b[A\n",
      " 34%|███▎      | 89/265 [00:01<00:02, 85.64it/s]\n",
      "Loading res3b.conv2._batch_norm._variance: \u001b[A\n",
      "Loading res3c.conv0._conv.weight:          \u001b[A\n",
      "Loading res3c.conv0._batch_norm.weight: \u001b[A\n",
      "Loading res3c.conv0._batch_norm.bias:   \u001b[A\n",
      "Loading res3c.conv0._batch_norm._mean: \u001b[A\n",
      "Loading res3c.conv0._batch_norm._variance: \u001b[A\n",
      "Loading res3c.conv1._conv.weight:          \u001b[A\n",
      "Loading res3c.conv1._batch_norm.weight: \u001b[A\n",
      " 37%|███▋      | 98/265 [00:01<00:01, 86.50it/s]\n",
      "Loading res3c.conv1._batch_norm._mean: \u001b[A\n",
      "Loading res3c.conv1._batch_norm._variance: \u001b[A\n",
      "Loading res3c.conv2._conv.weight:          \u001b[A\n",
      "Loading res3c.conv2._batch_norm.weight: \u001b[A\n",
      "Loading res3c.conv2._batch_norm.bias:   \u001b[A\n",
      "Loading res3c.conv2._batch_norm._mean: \u001b[A\n",
      "Loading res3c.conv2._batch_norm._variance: \u001b[A\n",
      "Loading res3d.conv0._conv.weight:          \u001b[A\n",
      "Loading res3d.conv0._batch_norm.weight: \u001b[A\n",
      " 41%|████      | 108/265 [00:01<00:01, 87.56it/s]\n",
      "Loading res3d.conv0._batch_norm._mean: \u001b[A\n",
      "Loading res3d.conv0._batch_norm._variance: \u001b[A\n",
      "Loading res3d.conv1._conv.weight:          \u001b[A\n",
      "Loading res3d.conv1._batch_norm.weight: \u001b[A\n",
      "Loading res3d.conv1._batch_norm.bias:   \u001b[A\n",
      "Loading res3d.conv1._batch_norm._mean: \u001b[A\n",
      "Loading res3d.conv1._batch_norm._variance: \u001b[A\n",
      "Loading res3d.conv2._conv.weight:          \u001b[A\n",
      " 44%|████▍     | 117/265 [00:01<00:01, 87.98it/s]\n",
      "Loading res3d.conv2._batch_norm.bias:   \u001b[A\n",
      "Loading res3d.conv2._batch_norm._mean: \u001b[A\n",
      "Loading res3d.conv2._batch_norm._variance: \u001b[A\n",
      "Loading res4a.conv0._conv.weight:          \u001b[A\n",
      "Loading res4a.conv0._batch_norm.weight: \u001b[A\n",
      "Loading res4a.conv0._batch_norm.bias:   \u001b[A\n",
      "Loading res4a.conv0._batch_norm._mean: \u001b[A\n",
      "Loading res4a.conv0._batch_norm._variance: \u001b[A\n",
      " 48%|████▊     | 126/265 [00:01<00:01, 88.27it/s]\n",
      "Loading res4a.conv1._batch_norm.weight: \u001b[A\n",
      "Loading res4a.conv1._batch_norm.bias:   \u001b[A\n",
      "Loading res4a.conv1._batch_norm._mean: \u001b[A\n",
      "Loading res4a.conv1._batch_norm._variance: \u001b[A\n",
      "Loading res4a.conv2._conv.weight:          \u001b[A\n",
      "Loading res4a.conv2._batch_norm.weight: \u001b[A\n",
      "Loading res4a.conv2._batch_norm.bias:   \u001b[A\n",
      "Loading res4a.conv2._batch_norm._mean: \u001b[A\n",
      " 51%|█████     | 135/265 [00:01<00:01, 88.53it/s]\n",
      "Loading res4a.short._conv.weight:          \u001b[A\n",
      "Loading res4a.short._batch_norm.weight: \u001b[A\n",
      "Loading res4a.short._batch_norm.bias:   \u001b[A\n",
      "Loading res4a.short._batch_norm._mean: \u001b[A\n",
      "Loading res4a.short._batch_norm._variance: \u001b[A\n",
      "Loading res4b.conv0._conv.weight:          \u001b[A\n",
      "Loading res4b.conv0._batch_norm.weight: \u001b[A\n",
      "Loading res4b.conv0._batch_norm.bias:   \u001b[A\n",
      " 54%|█████▍    | 144/265 [00:01<00:01, 88.58it/s]\n",
      "Loading res4b.conv0._batch_norm._variance: \u001b[A\n",
      "Loading res4b.conv1._conv.weight:          \u001b[A\n",
      "Loading res4b.conv1._batch_norm.weight: \u001b[A\n",
      "Loading res4b.conv1._batch_norm.bias:   \u001b[A\n",
      "Loading res4b.conv1._batch_norm._mean: \u001b[A\n",
      "Loading res4b.conv1._batch_norm._variance: \u001b[A\n",
      "Loading res4b.conv2._conv.weight:          \u001b[A\n",
      "Loading res4b.conv2._batch_norm.weight: \u001b[A\n",
      " 58%|█████▊    | 153/265 [00:01<00:01, 88.90it/s]\n",
      "Loading res4b.conv2._batch_norm._mean: \u001b[A\n",
      "Loading res4b.conv2._batch_norm._variance: \u001b[A\n",
      "Loading res4c.conv0._conv.weight:          \u001b[A\n",
      "Loading res4c.conv0._batch_norm.weight: \u001b[A\n",
      "Loading res4c.conv0._batch_norm.bias:   \u001b[A\n",
      "Loading res4c.conv0._batch_norm._mean: \u001b[A\n",
      "Loading res4c.conv0._batch_norm._variance: \u001b[A\n",
      "Loading res4c.conv1._conv.weight:          \u001b[A\n",
      " 61%|██████    | 162/265 [00:01<00:01, 88.37it/s]\n",
      "Loading res4c.conv1._batch_norm.bias:   \u001b[A\n",
      "Loading res4c.conv1._batch_norm._mean: \u001b[A\n",
      "Loading res4c.conv1._batch_norm._variance: \u001b[A\n",
      "Loading res4c.conv2._conv.weight:          \u001b[A\n",
      "Loading res4c.conv2._batch_norm.weight: \u001b[A\n",
      "Loading res4c.conv2._batch_norm.bias:   \u001b[A\n",
      "Loading res4c.conv2._batch_norm._mean: \u001b[A\n",
      "Loading res4c.conv2._batch_norm._variance: \u001b[A\n",
      " 65%|██████▍   | 171/265 [00:01<00:01, 87.56it/s]\n",
      "Loading res4d.conv0._batch_norm.weight: \u001b[A\n",
      "Loading res4d.conv0._batch_norm.bias:   \u001b[A\n",
      "Loading res4d.conv0._batch_norm._mean: \u001b[A\n",
      "Loading res4d.conv0._batch_norm._variance: \u001b[A\n",
      "Loading res4d.conv1._conv.weight:          \u001b[A\n",
      "Loading res4d.conv1._batch_norm.weight: \u001b[A\n",
      "Loading res4d.conv1._batch_norm.bias:   \u001b[A\n",
      "Loading res4d.conv1._batch_norm._mean: \u001b[A\n",
      " 68%|██████▊   | 180/265 [00:02<00:00, 87.51it/s]\n",
      "Loading res4d.conv2._conv.weight:          \u001b[A\n",
      "Loading res4d.conv2._batch_norm.weight: \u001b[A\n",
      "Loading res4d.conv2._batch_norm.bias:   \u001b[A\n",
      "Loading res4d.conv2._batch_norm._mean: \u001b[A\n",
      "Loading res4d.conv2._batch_norm._variance: \u001b[A\n",
      "Loading res4e.conv0._conv.weight:          \u001b[A\n",
      "Loading res4e.conv0._batch_norm.weight: \u001b[A\n",
      "Loading res4e.conv0._batch_norm.bias:   \u001b[A\n",
      " 71%|███████▏  | 189/265 [00:02<00:00, 87.36it/s]\n",
      "Loading res4e.conv0._batch_norm._variance: \u001b[A\n",
      "Loading res4e.conv1._conv.weight:          \u001b[A\n",
      "Loading res4e.conv1._batch_norm.weight: \u001b[A\n",
      "Loading res4e.conv1._batch_norm.bias:   \u001b[A\n",
      "Loading res4e.conv1._batch_norm._mean: \u001b[A\n",
      "Loading res4e.conv1._batch_norm._variance: \u001b[A\n",
      "Loading res4e.conv2._conv.weight:          \u001b[A\n",
      "Loading res4e.conv2._batch_norm.weight: \u001b[A\n",
      " 75%|███████▍  | 198/265 [00:02<00:00, 87.41it/s]\n",
      "Loading res4e.conv2._batch_norm._mean: \u001b[A\n",
      "Loading res4e.conv2._batch_norm._variance: \u001b[A\n",
      "Loading res4f.conv0._conv.weight:          \u001b[A\n",
      "Loading res4f.conv0._batch_norm.weight: \u001b[A\n",
      "Loading res4f.conv0._batch_norm.bias:   \u001b[A\n",
      "Loading res4f.conv0._batch_norm._mean: \u001b[A\n",
      "Loading res4f.conv0._batch_norm._variance: \u001b[A\n",
      "Loading res4f.conv1._conv.weight:          \u001b[A\n",
      " 78%|███████▊  | 207/265 [00:02<00:00, 86.90it/s]\n",
      "Loading res4f.conv1._batch_norm.bias:   \u001b[A\n",
      "Loading res4f.conv1._batch_norm._mean: \u001b[A\n",
      "Loading res4f.conv1._batch_norm._variance: \u001b[A\n",
      "Loading res4f.conv2._conv.weight:          \u001b[A\n",
      "Loading res4f.conv2._batch_norm.weight: \u001b[A\n",
      "Loading res4f.conv2._batch_norm.bias:   \u001b[A\n",
      "Loading res4f.conv2._batch_norm._mean: \u001b[A\n",
      "Loading res4f.conv2._batch_norm._variance: \u001b[A\n",
      " 82%|████████▏ | 216/265 [00:02<00:00, 85.39it/s]\n",
      "Loading res5a.conv0._batch_norm.weight: \u001b[A\n",
      "Loading res5a.conv0._batch_norm.bias:   \u001b[A\n",
      "Loading res5a.conv0._batch_norm._mean: \u001b[A\n",
      "Loading res5a.conv0._batch_norm._variance: \u001b[A\n",
      "Loading res5a.conv1._conv.weight:          \u001b[A\n",
      "Loading res5a.conv1._batch_norm.weight: \u001b[A\n",
      "Loading res5a.conv1._batch_norm.bias:   \u001b[A\n",
      "Loading res5a.conv1._batch_norm._mean: \u001b[A\n",
      " 85%|████████▍ | 225/265 [00:02<00:00, 85.27it/s]\n",
      "Loading res5a.conv2._conv.weight:          \u001b[A\n",
      "Loading res5a.conv2._batch_norm.weight: \u001b[A\n",
      "Loading res5a.conv2._batch_norm.bias:   \u001b[A\n",
      "Loading res5a.conv2._batch_norm._mean: \u001b[A\n",
      "Loading res5a.conv2._batch_norm._variance: \u001b[A\n",
      "Loading res5a.short._conv.weight:          \u001b[A\n",
      "Loading res5a.short._batch_norm.weight: \u001b[A\n",
      "Loading res5a.short._batch_norm.bias:   \u001b[A\n",
      " 88%|████████▊ | 234/265 [00:02<00:00, 85.38it/s]\n",
      "Loading res5a.short._batch_norm._variance: \u001b[A\n",
      "Loading res5b.conv0._conv.weight:          \u001b[A\n",
      "Loading res5b.conv0._batch_norm.weight: \u001b[A\n",
      "Loading res5b.conv0._batch_norm.bias:   \u001b[A\n",
      "Loading res5b.conv0._batch_norm._mean: \u001b[A\n",
      "Loading res5b.conv0._batch_norm._variance: \u001b[A\n",
      "Loading res5b.conv1._conv.weight:          \u001b[A\n",
      "Loading res5b.conv1._batch_norm.weight: \u001b[A\n",
      " 92%|█████████▏| 243/265 [00:02<00:00, 85.58it/s]\n",
      "Loading res5b.conv1._batch_norm._mean: \u001b[A\n",
      "Loading res5b.conv1._batch_norm._variance: \u001b[A\n",
      "Loading res5b.conv2._conv.weight:          \u001b[A\n",
      "Loading res5b.conv2._batch_norm.weight: \u001b[A\n",
      "Loading res5b.conv2._batch_norm.bias:   \u001b[A\n",
      "Loading res5b.conv2._batch_norm._mean: \u001b[A\n",
      "Loading res5b.conv2._batch_norm._variance: \u001b[A\n",
      "Loading res5c.conv0._conv.weight:          \u001b[A\n",
      " 95%|█████████▌| 252/265 [00:02<00:00, 85.67it/s]\n",
      "Loading res5c.conv0._batch_norm.bias:   \u001b[A\n",
      "Loading res5c.conv0._batch_norm._mean: \u001b[A\n",
      "Loading res5c.conv0._batch_norm._variance: \u001b[A\n",
      "Loading res5c.conv1._conv.weight:          \u001b[A\n",
      "Loading res5c.conv1._batch_norm.weight: \u001b[A\n",
      "Loading res5c.conv1._batch_norm.bias:   \u001b[A\n",
      "Loading res5c.conv1._batch_norm._mean: \u001b[A\n",
      "Loading res5c.conv1._batch_norm._variance: \u001b[A\n",
      " 98%|█████████▊| 261/265 [00:03<00:00, 85.31it/s]\n",
      "Loading res5c.conv2._batch_norm.weight: \u001b[A\n",
      "Loading res5c.conv2._batch_norm.bias:   \u001b[A\n",
      "Loading res5c.conv2._batch_norm._mean: \u001b[A\n",
      "100%|██████████| 265/265 [00:03<00:00, 85.94it/s]\n",
      "Loading res5c.conv2._batch_norm._variance: \n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:641: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[  1/10 ] train step:0 / 344.25 loss: 0.68812 lr: 0.001000 elapse: 1.016 reader: 0.861 top1: 0.50000s ips: 3.93704 instance/sec.\n",
      "epoch:[  1/10 ] train step:10 / 344.25 loss: 0.15976 lr: 0.001000 elapse: 2.660 reader: 2.534 top1: 1.00000s ips: 1.50360 instance/sec.\n",
      "epoch:[  1/10 ] train step:20 / 344.25 loss: 0.29059 lr: 0.001000 elapse: 1.666 reader: 1.539 top1: 0.75000s ips: 2.40090 instance/sec.\n",
      "epoch:[  1/10 ] train step:30 / 344.25 loss: 0.08492 lr: 0.001000 elapse: 1.884 reader: 1.760 top1: 1.00000s ips: 2.12275 instance/sec.\n",
      "epoch:[  1/10 ] train step:40 / 344.25 loss: 0.05763 lr: 0.001000 elapse: 0.340 reader: 0.212 top1: 1.00000s ips: 11.75324 instance/sec.\n",
      "epoch:[  1/10 ] train step:50 / 344.25 loss: 1.64876 lr: 0.001000 elapse: 1.870 reader: 1.746 top1: 0.50000s ips: 2.13850 instance/sec.\n",
      "epoch:[  1/10 ] train step:60 / 344.25 loss: 1.43403 lr: 0.001000 elapse: 3.113 reader: 2.988 top1: 0.50000s ips: 1.28488 instance/sec.\n",
      "epoch:[  1/10 ] train step:70 / 344.25 loss: 0.46399 lr: 0.001000 elapse: 0.623 reader: 0.493 top1: 0.75000s ips: 6.42253 instance/sec.\n",
      "epoch:[  1/10 ] train step:80 / 344.25 loss: 0.06273 lr: 0.001000 elapse: 3.632 reader: 3.506 top1: 1.00000s ips: 1.10135 instance/sec.\n",
      "epoch:[  1/10 ] train step:90 / 344.25 loss: 0.84688 lr: 0.001000 elapse: 1.355 reader: 1.209 top1: 0.50000s ips: 2.95237 instance/sec.\n",
      "epoch:[  1/10 ] train step:100 / 344.25 loss: 0.46800 lr: 0.001000 elapse: 0.694 reader: 0.567 top1: 0.50000s ips: 5.75963 instance/sec.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_179/2848453312.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 在执行代码过程中，如果出现 ‘ValueError: parameter name [conv1_weights] have be been used’ 问题，\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# 可以点击上方的第三个按钮 ‘重启并运行全部’ 来解决\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_179/1026118799.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(validate)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mrecord_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0mrecord_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reader_time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dataloader/dataloader_iter.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0min_dygraph_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_next_var_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_restore_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_structure_infos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 在执行代码过程中，如果出现 ‘ValueError: parameter name [conv1_weights] have be been used’ 问题，\n",
    "# 可以点击上方的第三个按钮 ‘重启并运行全部’ 来解决\n",
    "train_model(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_model(weights):\n",
    "    # 1. Construct model\n",
    "    tsm = ResNetTSM(pretrained=None,\n",
    "                    layers=layers,\n",
    "                    num_seg=num_seg)\n",
    "    head = TSMHead(num_classes=num_classes,\n",
    "                   in_channels=in_channels,\n",
    "                   drop_ratio=drop_ratio)\n",
    "    model = Recognizer2D(backbone=tsm, head=head)\n",
    "\n",
    "    # 2. Construct dataset and dataloader.\n",
    "    test_pipeline = Compose(train_mode=False)\n",
    "    test_dataset = VideoDataset(file_path=valid_file_path,\n",
    "                                     pipeline=test_pipeline,\n",
    "                                     suffix=suffix)\n",
    "    test_sampler = paddle.io.DistributedBatchSampler(test_dataset,\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        shuffle=valid_shuffle,\n",
    "                                                        drop_last=True)\n",
    "    test_loader = paddle.io.DataLoader(test_dataset,\n",
    "                                        batch_sampler=test_sampler,\n",
    "                                        places=paddle.set_device('gpu'),\n",
    "                                        return_list=return_list)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    state_dicts = paddle.load(weights)\n",
    "    model.set_state_dict(state_dicts)\n",
    "\n",
    "    # add params to metrics\n",
    "    data_size = len(test_dataset)\n",
    "    \n",
    "    metric = CenterCropMetric(data_size=data_size, batch_size=batch_size)\n",
    "    for batch_id, data in enumerate(test_loader):\n",
    "        outputs = model.test_step(data)\n",
    "        metric.update(batch_id, data, outputs)\n",
    "    metric.accumulate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0115 23:26:01.676389 22490 device_context.cc:404] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 10.1, Runtime API Version: 10.1\n",
      "W0115 23:26:01.681787 22490 device_context.cc:422] device: 0, cuDNN Version: 7.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] Processing batch 0/294 ...\n",
      "[TEST] Processing batch 20/294 ...\n",
      "[TEST] Processing batch 40/294 ...\n",
      "[TEST] Processing batch 60/294 ...\n",
      "[TEST] Processing batch 80/294 ...\n",
      "[TEST] Processing batch 100/294 ...\n",
      "[TEST] Processing batch 120/294 ...\n",
      "[TEST] Processing batch 140/294 ...\n",
      "[TEST] Processing batch 160/294 ...\n",
      "[TEST] Processing batch 180/294 ...\n",
      "[TEST] Processing batch 200/294 ...\n",
      "[TEST] Processing batch 220/294 ...\n",
      "[TEST] Processing batch 240/294 ...\n",
      "[TEST] Processing batch 260/294 ...\n",
      "[TEST] Processing batch 280/294 ...\n",
      "[TEST] finished, avg_acc1= 0.9812925457954407, avg_acc5= 0.0 \n"
     ]
    }
   ],
   "source": [
    "# 验证集进行测试\n",
    "# 在执行代码过程中，如果出现 ‘ValueError: parameter name [conv1_weights] have be been used’ 问题，\n",
    "# 可以点击上方的第三个按钮 ‘重启并运行全部’ 来解决\n",
    "model_file = './output/TSM/TSM_best.pdparams'\n",
    "test_model(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0115 23:30:21.339340  4658 device_context.cc:404] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 10.1, Runtime API Version: 10.1\n",
      "W0115 23:30:21.344605  4658 device_context.cc:422] device: 0, cuDNN Version: 7.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "真实类别：0, 模型预测类别：0\n",
      "真实类别：0, 模型预测类别：0\n",
      "真实类别：0, 模型预测类别：0\n",
      "真实类别：1, 模型预测类别：1\n",
      "真实类别：0, 模型预测类别：0\n",
      "真实类别：0, 模型预测类别：0\n",
      "真实类别：0, 模型预测类别：0\n"
     ]
    }
   ],
   "source": [
    "def inference(model_file):\n",
    "    # 1. Construct model\n",
    "    tsm = ResNetTSM(pretrained=None,\n",
    "                    layers=layers,\n",
    "                    num_seg=num_seg)\n",
    "    head = TSMHead(num_classes=num_classes,\n",
    "                   in_channels=in_channels,\n",
    "                   drop_ratio=drop_ratio)\n",
    "    model = Recognizer2D(backbone=tsm, head=head)\n",
    "\n",
    "    # 2. Construct dataset and dataloader.\n",
    "    test_pipeline = Compose(train_mode=False)\n",
    "    test_dataset = VideoDataset(file_path=valid_file_path,\n",
    "                                     pipeline=test_pipeline,\n",
    "                                     suffix=suffix)\n",
    "    test_sampler = paddle.io.DistributedBatchSampler(test_dataset,\n",
    "                                                     batch_size=1,\n",
    "                                                     shuffle=True,\n",
    "                                                     drop_last=True)\n",
    "    test_loader = paddle.io.DataLoader(test_dataset,\n",
    "                                       batch_sampler=test_sampler,\n",
    "                                       places=paddle.set_device('gpu'),\n",
    "                                       return_list=return_list)\n",
    "\n",
    "    model.eval()\n",
    "    state_dicts = paddle.load(model_file)\n",
    "    model.set_state_dict(state_dicts)\n",
    "\n",
    "    for batch_id, data in enumerate(test_loader):\n",
    "        _, labels = data\n",
    "        outputs = model.test_step(data)\n",
    "        scores = F.softmax(outputs)\n",
    "        class_id = paddle.argmax(scores, axis=-1)\n",
    "        pred = class_id.numpy()[0]\n",
    "        label = labels.numpy()[0][0]\n",
    "        \n",
    "        print('真实类别：{}, 模型预测类别：{}'.format(pred, label))\n",
    "        if batch_id > 5:\n",
    "            break\n",
    "\n",
    "# 启动推理\n",
    "# 在执行代码过程中，如果出现 ‘ValueError: parameter name [conv1_weights] have be been used’ 问题，\n",
    "# 可以点击上方的第三个按钮 ‘重启并运行全部’ 来解决\n",
    "model_file = './output/TSM/TSM_best.pdparams'\n",
    "inference(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
